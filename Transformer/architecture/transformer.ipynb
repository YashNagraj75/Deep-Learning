{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport time\nimport numpy as np \nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras.layers import MultiHeadAttention, Embedding, Dense, Input, Dropout, LayerNormalization\nfrom transformers import DistilBertTokenizerFast \nfrom transformers import TFDistilBertForTokenClassification","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-15T06:42:46.898719Z","iopub.execute_input":"2024-06-15T06:42:46.899116Z","iopub.status.idle":"2024-06-15T06:43:06.963865Z","shell.execute_reply.started":"2024-06-15T06:42:46.899086Z","shell.execute_reply":"2024-06-15T06:43:06.962645Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-06-15 06:42:48.748243: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-15 06:42:48.748354: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-15 06:42:48.882398: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Positional Encodings \nWe calculate the positional encodings and add it to the input sequence as an input the encoder and decoder inorder to get the specific poistion of the word in the setence.\n\nThe formula being:\n$$\nPE_{(pos, 2i)}= sin\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n\\tag{1}$$\n<br>\n$$\nPE_{(pos, 2i+1)}= cos\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n\\tag{2}$$\n\n* $d$ is the dimension of the word embedding and positional encoding\n* $pos$ is the position of the word.\n* $k$ refers to each of the different dimensions in the positional encodings, with $i$ equal to $k$ $//$ $2$.","metadata":{}},{"cell_type":"code","source":"# Function to get the angles\ndef get_angles(pos,k,d):\n    i = k//2\n    \n    angles = pos / np.power(10000 ,(2*i)/ np.float32(d))\n    \n    return angles","metadata":{"execution":{"iopub.status.busy":"2024-06-15T06:43:06.965868Z","iopub.execute_input":"2024-06-15T06:43:06.966543Z","iopub.status.idle":"2024-06-15T06:43:06.972858Z","shell.execute_reply.started":"2024-06-15T06:43:06.966511Z","shell.execute_reply":"2024-06-15T06:43:06.971605Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Function to calculate the positional encodings\ndef positional_encodings(positions,d):\n    angle_rads = get_angles(np.arange(positions)[:,np.newaxis],\n                            np.arange(d)[np.newaxis,:],\n                            d)\n    \n    angle_rads[:,0::2] = np.sin(angle_rads[:,0::2])\n    angle_rads[:,1::2] = np.cos(angle_rads[:,1::2])\n    \n    pos_encoding = angle_rads[np.newaxis,...]\n    \n    return tf.cast(pos_encoding,dtype= tf.float32)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-15T06:43:06.974100Z","iopub.execute_input":"2024-06-15T06:43:06.974424Z","iopub.status.idle":"2024-06-15T06:43:06.985273Z","shell.execute_reply.started":"2024-06-15T06:43:06.974398Z","shell.execute_reply":"2024-06-15T06:43:06.984214Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"pe = positional_encodings(50,512)\nprint(pe.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T06:43:06.988071Z","iopub.execute_input":"2024-06-15T06:43:06.988443Z","iopub.status.idle":"2024-06-15T06:43:07.547757Z","shell.execute_reply.started":"2024-06-15T06:43:06.988412Z","shell.execute_reply":"2024-06-15T06:43:07.546431Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"(1, 50, 512)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2. Masking\n\nWe will use 2 types of masks which are:\n- Padding Mask : This adds a boolean mask after as to which entry in the padding sequence must be addressed and which not especially the zeros.\n- Look Ahead Mask : This helps the model pretend that it correctly predicted a part of the output and see if, without looking ahead, it can correctly predict the next output. ","metadata":{}},{"cell_type":"code","source":"def padding_mask(decoder_token_ids):\n    seq = 1 - tf.cast(tf.math.equal(decoder_token_ids,0),dtype=tf.float32)\n    \n    return seq[:,tf.newaxis,:]\n\nx = tf.constant([[7., 6., 0., 0., 1.], [1., 2., 3., 0., 0.], [0., 0., 0., 4., 5.]])\nprint(padding_mask(x))","metadata":{"execution":{"iopub.status.busy":"2024-06-15T06:43:07.549060Z","iopub.execute_input":"2024-06-15T06:43:07.549416Z","iopub.status.idle":"2024-06-15T06:43:07.577815Z","shell.execute_reply.started":"2024-06-15T06:43:07.549386Z","shell.execute_reply":"2024-06-15T06:43:07.576753Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"tf.Tensor(\n[[[1. 1. 0. 0. 1.]]\n\n [[1. 1. 1. 0. 0.]]\n\n [[0. 0. 0. 1. 1.]]], shape=(3, 1, 5), dtype=float32)\n","output_type":"stream"}]},{"cell_type":"code","source":"def look_ahead_mask(sequence_length):\n    mask = tf.linalg.band_part(tf.ones((1, sequence_length, sequence_length)), -1, 0)\n    return mask","metadata":{"execution":{"iopub.status.busy":"2024-06-15T06:43:07.579204Z","iopub.execute_input":"2024-06-15T06:43:07.579952Z","iopub.status.idle":"2024-06-15T06:43:07.585253Z","shell.execute_reply.started":"2024-06-15T06:43:07.579912Z","shell.execute_reply":"2024-06-15T06:43:07.584262Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"x = tf.random.uniform((1, 3))\ntemp = look_ahead_mask(x.shape[1])\ntemp","metadata":{"execution":{"iopub.status.busy":"2024-06-15T06:43:07.586787Z","iopub.execute_input":"2024-06-15T06:43:07.587192Z","iopub.status.idle":"2024-06-15T06:43:07.616665Z","shell.execute_reply.started":"2024-06-15T06:43:07.587157Z","shell.execute_reply":"2024-06-15T06:43:07.615625Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\narray([[[1., 0., 0.],\n        [1., 1., 0.],\n        [1., 1., 1.]]], dtype=float32)>"},"metadata":{}}]},{"cell_type":"code","source":"def scaled_dot_product_attention(q,v,k, mask):\n    mat_mul = tf.matmul(q,v,transpose_b=True)\n    \n    dk = tf.cast(tf.shape(k)[-1], tf.absfloat32)\n    \n    scaled_attention_logits = mat_mul / dk\n    \n    if mask is None:\n        scaled_attention += ((1 - mask) * -1e9)\n        \n    attention_weights = tf.nn.softmax(scaled_attention_logits,axis=-1)\n    \n    outputs = tf.matmul(attention_weights,v)\n    \n    return outputs,attention_weights","metadata":{"execution":{"iopub.status.busy":"2024-06-15T06:43:07.617814Z","iopub.execute_input":"2024-06-15T06:43:07.618106Z","iopub.status.idle":"2024-06-15T06:43:07.625158Z","shell.execute_reply.started":"2024-06-15T06:43:07.618081Z","shell.execute_reply":"2024-06-15T06:43:07.624232Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# The Encoder","metadata":{}},{"cell_type":"code","source":"# This is the feed forward network \ndef FullyConnected(embedding_dim, fully_connected_dim):\n    return tf.keras.Sequential([\n        tf.keras.layers.Dense(fully_connected_dim,activation='relu'),\n        tf.keras.layers.Dense(embedding_dim)\n    ])","metadata":{"execution":{"iopub.status.busy":"2024-06-15T06:43:07.626655Z","iopub.execute_input":"2024-06-15T06:43:07.627395Z","iopub.status.idle":"2024-06-15T06:43:07.634858Z","shell.execute_reply.started":"2024-06-15T06:43:07.627363Z","shell.execute_reply":"2024-06-15T06:43:07.633926Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class EncodeLayer(tf.keras.layers.Layer):\n    def __init__(self,embedding_dim,num_heads, fully_connected_dim,\n                dropout_rate=0.6, layernorm_eps = 1e-6):\n        super(EncoderLayer, self).__init__()\n        \n        self.mha = MultiHeadAttention(num_heads = num_heads,\n                                     key_dim = embedding_dim,\n                                     dropout = dropout_rate)\n        \n        self.ffn = FullyConnected(embedding_dim,fully_connected_dim)\n        \n        self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)\n        self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)\n        \n        self.dropout_ffn = Dropout(rate=dropout_rate)\n        \n    def call(self,x,training,mask):\n        \n        self_mha_output = self.mha(x,x,x,mask) # Gives the multihead attention logits\n        \n        skip_x_attention = self.layernorm1(x + self_mha_output) # Here we apply the add and norm layer for better performance\n        \n        ffn_output = self.ffn(skip_x_attention) # Pass the logits from mha to ffn\n        \n        ffn_output = self.dropout_ffn(fnn_output,training = training) # Use dropout on the computed weights\n        \n        encoder_layer_output = self.layernorm2(ffn_output+skip_x_attention)\n        \n\n        return encode_layer_output\n        ","metadata":{"execution":{"iopub.status.busy":"2024-06-15T06:43:07.637602Z","iopub.execute_input":"2024-06-15T06:43:07.637939Z","iopub.status.idle":"2024-06-15T06:43:07.649790Z","shell.execute_reply.started":"2024-06-15T06:43:07.637912Z","shell.execute_reply":"2024-06-15T06:43:07.648867Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Now the full Encoding layer using positional encodings\nclass Encoder(tf.keras.layers.Layer):\n    def __init__(self,num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size,\n               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n        super(Encoder, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.fully_connected_dim = fully_connected_dim\n        self.num_layer = num_layers\n        \n        self.embedding =Embedding(input_dim = input_vocab_size,output_dim = self.embedding_dim)\n        \n        self.encode_layers= [EncodeLayer(self.embedding_dim,\n                                    num_heads = num_heads,\n                                    fully_connected_dim = fully_connected_dim,\n                                    dropout_rate = dropout_rate,\n                                    layernorm_eps = layernorm_eps)\n                            for _ in range(self.num_layers)] \n        self.pos_encoding = positional_encoding(maximum_position_encoding, \n                                                self.embedding_dim)\n        \n        self.dropout = Dropout(dropout_layer)\n        \n    \n    def call(self,x,trainig,mask):\n        seq_len = tf.shape(x)[1]\n        \n        x = self.embedding(x)\n        \n        x*= tf.math.sqrt(tf.cast(self.embedding_dim,tf.float32)) # This is to scale the embedding with the sqrt(dimensions of the embeddings)\n        \n        x += self.pos_encoding[:, :seq_len,:]# Add the positional encodings to the embeddings\n        \n        x = self.dropout(x,training = training)\n        \n        for i in range(self.num_layers):\n            x = self.encode_layers[i](x,training,mask)\n            \n        return x\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2024-06-15T06:43:07.651120Z","iopub.execute_input":"2024-06-15T06:43:07.651488Z","iopub.status.idle":"2024-06-15T06:43:07.663041Z","shell.execute_reply.started":"2024-06-15T06:43:07.651460Z","shell.execute_reply":"2024-06-15T06:43:07.662146Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class DecodeLayer(tf.keras.layers.Layer):\n    def __init__(self,embedding_dim,full_connected_dim,num_heads,dropout_rate=0.1,layernorm_eps = 1e-6):\n        \n        self.mha1 = MultiHeadAttention(num_heads,\n                                      key_dim = embedding_dim,\n                                      dropout = dropout_rate)\n        \n        self.mha2 = MultiHeadAttention(num_heads,\n                                      key_dim = embedding_dim,\n                                      dropout = dropout_rate)\n        \n        self.ffn = FullyConnected(embedding_dim = embedding_dim,\n                                 fully_connected_dim = fully_connected_dim)\n        \n        self.layernorm1 = LayerNormalization(epsilon = layernorm_eps)\n        self.layernorm2 = LayerNormalization(epsilon = layernorm_eps)\n        self.layernorm3 = LayerNormalization(epsilon = layernorm_eps)\n        \n        self.DropoutFFN = Dropout(dropout_rate)\n        \n        \n    def call(self,x,encoder_output,training,padding_mask,look_ahead_mask):\n        \n        mha_out_1, attn_weights_1 = self.mha1(x,x,x,look_ahead_mask,return_attention_scores=True)\n        \n        Q1 = self.layernorm1(x + mha_out_1) # Apply normalization layer to the sum of input and mha_output\n        \n        mha_out_2 , attn_weights_2 = self.mha2(Q1,encoder_output,encoder_output,padding_mask, return_attention_scores=True)\n        \n        mha_out_2 = self.layernorm2(Q1 + mha_out_2)\n        \n        ffn = self.ffn(mha_out_2)\n        \n        ffn = self.DropoutFFN(ffn,training=training)\n        \n        out3 = self.layernorm3(ffn + mha_out_2)\n        \n        \n        return out3,attn_weights_1, attn_weights_2  ","metadata":{"execution":{"iopub.status.busy":"2024-06-15T07:26:23.678375Z","iopub.execute_input":"2024-06-15T07:26:23.678775Z","iopub.status.idle":"2024-06-15T07:26:23.690144Z","shell.execute_reply.started":"2024-06-15T07:26:23.678743Z","shell.execute_reply":"2024-06-15T07:26:23.688960Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class Decoder(tf.keras.layers.Layer):\n    def __init__(self,num_layers,num_heads,embedding_dim,fully_connected_dim,target_vocab_size,\n                max_position_encoding,dropout_rate=0.1,layer_eps=1e-6):\n        self.num_layer = num_layers\n        self.embedding_dim = embedding_dim,\n        \n        self.embedding = Embedding(target_vocab_size,embedding_dim)\n        self.positional_encoding = positional_encoding(max_positional_encoding,embedding_dim)\n        \n        self.dec_layers = [\n            DecoderLayer(self.embedding_dim,\n                        fully_connected_dim,\n                        dropout_rate=dropout_rate,\n                        layernorm_eps = layernorm_eps)\n            for _ in range(self.num_layers)\n        ]\n        \n        self.dropout = Dropout(dropout_rate)\n        \n        \n    def call(self,x,enc_output,training,padding_mask,look_ahead_mask):\n        seq_len = tf.shape(x)[1]\n        attention_weights = {}\n        \n        x = self.embedding(x)\n        \n        x *= tf.math.sqrt(tf.cast(self.embedding_dim,tf.float32))\n        \n        x += self.positional_encoding[:,:seq_len,:]\n        \n        x = self.dropout(x,training=training)\n        \n        for i in range(self.num_layers):\n            x , block1, block2 = self.dec_layers[i](x,enc_output,training,look_ahead_mask,padding_mask)\n            \n            attention_weights['decoder_layer{}_block1_self_att'.format(i+1)] = block1\n            attention_weights['decoder_layer{}_block2_decenc_att'.format(i+1)] = block2\n            \n        return x , attention_weights","metadata":{"execution":{"iopub.status.busy":"2024-06-15T08:03:38.334543Z","iopub.execute_input":"2024-06-15T08:03:38.334973Z","iopub.status.idle":"2024-06-15T08:03:38.347053Z","shell.execute_reply.started":"2024-06-15T08:03:38.334920Z","shell.execute_reply":"2024-06-15T08:03:38.345924Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class Transformer(tf.keras.Model):\n    \"\"\"\n    Complete transformer with an Encoder and a Decoder\n    \"\"\"\n    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, \n               target_vocab_size, max_positional_encoding_input,\n               max_positional_encoding_target, dropout_rate=0.1, layernorm_eps=1e-6):\n        super(Transformer, self).__init__()\n\n        self.encoder = Encoder(num_layers=num_layers,\n                               embedding_dim=embedding_dim,\n                               num_heads=num_heads,\n                               fully_connected_dim=fully_connected_dim,\n                               input_vocab_size=input_vocab_size,\n                               maximum_position_encoding=max_positional_encoding_input,\n                               dropout_rate=dropout_rate,\n                               layernorm_eps=layernorm_eps)\n\n        self.decoder = Decoder(num_layers=num_layers, \n                               embedding_dim=embedding_dim,\n                               num_heads=num_heads,\n                               fully_connected_dim=fully_connected_dim,\n                               target_vocab_size=target_vocab_size, \n                               maximum_position_encoding=max_positional_encoding_target,\n                               dropout_rate=dropout_rate,\n                               layernorm_eps=layernorm_eps)\n\n        self.final_layer = Dense(target_vocab_size, activation='softmax')\n    \n    def call(self, input_sentence, output_sentence, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n        enc_output = self.encoder(input_sentence,training,enc_padding_mask)\n        \n        # call self.decoder with the appropriate arguments to get the decoder output\n        # dec_output.shape == (batch_size, tar_seq_len, embedding_dim)\n        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n        \n        # pass decoder output through a linear layer and softmax (~2 lines)\n        final_output = self.final_layer(dec_output) # (batch_size, tar_seq_len, target_vocab_size)\n        # END CODE HERE\n\n        return final_output, attention_weights","metadata":{"execution":{"iopub.status.busy":"2024-06-15T08:06:19.410347Z","iopub.execute_input":"2024-06-15T08:06:19.411163Z","iopub.status.idle":"2024-06-15T08:06:19.422592Z","shell.execute_reply.started":"2024-06-15T08:06:19.411100Z","shell.execute_reply":"2024-06-15T08:06:19.421463Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}